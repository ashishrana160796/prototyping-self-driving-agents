{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deliverable_milestone_three.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNMaSFqWE2m6"
      },
      "source": [
        "### **Creating MCTSAgent that works for multiple _highway-env_ environments.**\n",
        "\n",
        "In this milestone we intend to develop an agent that stores the trained MCTS tree for multiple _highway-env_ environment tasks. With each tree node having approximated UCT bound values after the training completion."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bca_Qh6TGYgp",
        "outputId": "a0cbb0f4-8479-4cf0-fe0a-6cc525e6ba47"
      },
      "source": [
        "# Package download statement for highway-env only.\n",
        "# We are not adding visualization import for this deliverable.\n",
        "!pip install git+https://github.com/eleurent/highway-env"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/eleurent/highway-env\n",
            "  Cloning https://github.com/eleurent/highway-env to /tmp/pip-req-build-9qujnive\n",
            "  Running command git clone -q https://github.com/eleurent/highway-env /tmp/pip-req-build-9qujnive\n",
            "Requirement already satisfied (use --upgrade to upgrade): highway-env==1.0.dev0 from git+https://github.com/eleurent/highway-env in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from highway-env==1.0.dev0) (0.17.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from highway-env==1.0.dev0) (1.19.4)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.6/dist-packages (from highway-env==1.0.dev0) (2.0.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from highway-env==1.0.dev0) (3.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from highway-env==1.0.dev0) (1.1.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym->highway-env==1.0.dev0) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym->highway-env==1.0.dev0) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->highway-env==1.0.dev0) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->highway-env==1.0.dev0) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->highway-env==1.0.dev0) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->highway-env==1.0.dev0) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->highway-env==1.0.dev0) (0.10.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->highway-env==1.0.dev0) (2018.9)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->highway-env==1.0.dev0) (0.16.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib->highway-env==1.0.dev0) (1.15.0)\n",
            "Building wheels for collected packages: highway-env\n",
            "  Building wheel for highway-env (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for highway-env: filename=highway_env-1.0.dev0-cp36-none-any.whl size=80900 sha256=2540c73fff3719702eea202162d33b34ab547ec4fa346c148ec398bf08e9d676\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-61_hx32v/wheels/e6/10/d8/02a077ca221bbac1c6fc12c1370c2f773a8cd602d4be3df0cc\n",
            "Successfully built highway-env\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVjs1g7JGhcA"
      },
      "source": [
        "# Imports required for MCTS base code execution.\n",
        "import sys\n",
        "import random\n",
        "import itertools\n",
        "from time import time\n",
        "from copy import copy, deepcopy\n",
        "from math import sqrt, log\n",
        "# Imports required for environment build and interactions.\n",
        "import gym\n",
        "import highway_env"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwaoVebFGhiN"
      },
      "source": [
        "# Defining node class and associated required properties with it.\n",
        "class Node:\n",
        "    def __init__(self, parent, action):\n",
        "        self.parent = parent\n",
        "        self.action = action\n",
        "        self.children = []\n",
        "        self.explored_children = 0\n",
        "        self.visits = 0\n",
        "        self.value = 0"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gS9LxTKLUySV"
      },
      "source": [
        "# HighwayAgent class creation that'll store the parent trained nodes for trained\n",
        "# MCTS agents for a given instance. Adding parent objects for all environments.\n",
        "class HighwayEnvAgent:\n",
        "    def __init__(self):\n",
        "        self.roundabout_root = Node(None, None)\n",
        "        self.merge_root = Node(None, None)\n",
        "        self.intersection_root = Node(None, None)\n",
        "        self.lane_change_root = Node(None, None)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTl8DRh7UzPW"
      },
      "source": [
        "# Declaring instance of HighwayEnvAgent Agent class.\n",
        "mcts_agent = HighwayEnvAgent()"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zxn8_zgvGhk2"
      },
      "source": [
        "# Basic utility functions for MCTS algorithm implementation.\n",
        "\n",
        "# This function determine complete exhaustive list of all the nodes.\n",
        "def node_expansion(space):\n",
        "    if isinstance(space, gym.spaces.Discrete):\n",
        "        return range(space.n)\n",
        "    elif isinstance(space, gym.spaces.Tuple):\n",
        "        return itertools.product(*[node_expansion(s) for s in space.spaces])\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "# Upper Confidence Bound U(s,a) calculation formula.\n",
        "def upper_conf_bound(node):\n",
        "    return node.value / node.visits + sqrt(log(node.parent.visits)/node.visits)\n",
        "\n",
        "# Calculates the averaged out value of the total reward gains for better\n",
        "# policy estimation during evaluation of search space by MCTS algorithm.\n",
        "def moving_averages(v, n):\n",
        "    n = min(len(v), n)\n",
        "    ret = [.0]*(len(v)-n+1)\n",
        "    ret[0] = float(sum(v[:n]))/n\n",
        "    for i in range(len(v)-n):\n",
        "        ret[i+1] = ret[i] + float(v[n+i] - v[i])/n\n",
        "    return ret"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQPLuPQxGhnu"
      },
      "source": [
        "# Executer function for training the agent and evaluating its performance as well.\n",
        "class Executer:\n",
        "    def __init__(self, env_name, num_execs=10, max_tree_depth=1000, episodes=10000, eval_flag=False):\n",
        "        self.env_name = env_name\n",
        "        self.num_execs = num_execs\n",
        "        self.max_tree_depth = max_tree_depth\n",
        "        self.episodes = episodes\n",
        "        self.eval_flag = eval_flag\n",
        "    \n",
        "    def print_stats(self, num_exec, score, avg_time, eval_flag):\n",
        "        sys.stdout.write('execution number: \\r%3d   total reward:%10.3f   average time:%4.1f s' % (num_exec, score, avg_time))\n",
        "        sys.stdout.flush()\n",
        "        if (num_exec % 10) == 0:\n",
        "            print(\"execution number: \\r%4d   total reward: %4.3f   average time: %4.2f s\" % (num_exec, score, avg_time))\n",
        "        elif num_exec == 1 and eval_flag == True:\n",
        "            print(\"execution number: \\r%4d   total reward: %4.3f   average time: %4.2f s\" % (num_exec, score, avg_time))\n",
        "\n",
        "    def execute(self):\n",
        "        print(self.env_name)\n",
        "        # For maintaining list of best rewards.\n",
        "        best_rewards = []\n",
        "        start_time = time()\n",
        "        env = gym.make(self.env_name)\n",
        "\n",
        "        for loop in range(self.num_execs):\n",
        "            env.reset()\n",
        "            root = Node(None, None)\n",
        "                                     \n",
        "            # Before executing this snippet make sure\n",
        "            # the search execution step has happened before.\n",
        "            if self.eval_flag == True:\n",
        "                if self.env_name == 'highway-v0':\n",
        "                    root = mcts_agent.lane_change_root\n",
        "                elif self.env_name == 'merge-v0':\n",
        "                    root = mcts_agent.merge_root\n",
        "                elif self.env_name == 'intersection-v0':\n",
        "                    root = mcts_agent.intersection_root\n",
        "                else:\n",
        "                    root = mcts_agent.roundabout_root\n",
        "\n",
        "            # For capturing list of best actions taken by the agent.\n",
        "            best_actions = []\n",
        "            best_reward = float(\"-inf\") \n",
        "\n",
        "            for _ in range(self.episodes):\n",
        "                state = copy(env)\n",
        "\n",
        "                sum_reward = 0\n",
        "                node = root\n",
        "                terminal = False\n",
        "                actions = []\n",
        "                \n",
        "                # The search execution step should be executed before.\n",
        "                if self.eval_flag == False:\n",
        "                    if self.env_name == 'highway-v0':\n",
        "                        mcts_agent.lane_change_root = node\n",
        "                    elif self.env_name == 'merge-v0':\n",
        "                        mcts_agent.merge_root = node\n",
        "                    elif self.env_name == 'intersection-v0':\n",
        "                        mcts_agent.intersection_root = node\n",
        "                    else:\n",
        "                        mcts_agent.roundabout_root = node\n",
        "                \n",
        "                # Selection of suitable node children.\n",
        "                while node.children:\n",
        "                    if node.explored_children < len(node.children):\n",
        "                        child = node.children[node.explored_children]\n",
        "                        node.explored_children += 1\n",
        "                        node = child\n",
        "                    else:\n",
        "                        node = max(node.children, key=upper_conf_bound)\n",
        "                    _, reward, terminal, _ = state.step(node.action)\n",
        "                    sum_reward += reward\n",
        "                    actions.append(node.action)\n",
        "\n",
        "                # Expansion of all the children nodes.\n",
        "                if not terminal:\n",
        "                    node.children = [Node(node, a) for a in node_expansion(state.action_space)]\n",
        "                    random.shuffle(node.children)\n",
        "\n",
        "                # Creating exhaustive list of actions.\n",
        "                while not terminal:\n",
        "                    action = state.action_space.sample()\n",
        "                    _, reward, terminal, _ = state.step(action)\n",
        "                    sum_reward += reward\n",
        "                    actions.append(action)\n",
        "\n",
        "                    if len(actions) > self.max_tree_depth:\n",
        "                        sum_reward -= 100\n",
        "                        break\n",
        "\n",
        "                # Retaining the best reward value and actions.\n",
        "                if best_reward < sum_reward:\n",
        "                    best_reward = sum_reward\n",
        "                    best_actions = actions\n",
        "\n",
        "                # Backpropagating in MCTS for assigning reward value to a node.\n",
        "                while node:\n",
        "                    node.visits += 1\n",
        "                    node.value += sum_reward\n",
        "                    node = node.parent\n",
        "\n",
        "            sum_reward = 0\n",
        "\n",
        "            # best_actions list stores the estimated action\n",
        "            # policy post the episode execution loop.\n",
        "            print(\"\\n execution number: \"+ str(loop+1) + \" || Corresponding best action sequence: \"+ str(best_actions) +\"\\n\")\n",
        "            for action in best_actions:\n",
        "                _, reward, terminal, _ = env.step(action)\n",
        "                sum_reward += reward\n",
        "                if terminal:\n",
        "                    break\n",
        "\n",
        "            best_rewards.append(sum_reward)\n",
        "            score = max(moving_averages(best_rewards, 100))\n",
        "            avg_time = (time() - start_time) / (loop + 1)\n",
        "            self.print_stats(loop + 1, score, avg_time, self.eval_flag)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GU4s7QcGhw1"
      },
      "source": [
        "# Running executer instance of training and evaluating the agent for\n",
        "# every discrete environment in highway-env package.\n",
        "Executer(env_name='highway-v0', num_execs=10, max_tree_depth=100, episodes=5000, eval_flag=False).execute()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FKPYfzbaFlR"
      },
      "source": [
        "Executer(env_name='highway-v0', num_execs=1, max_tree_depth=100, episodes=5000, eval_flag=True).execute()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scC97NQaad9C"
      },
      "source": [
        "Executer(env_name='merge-v0', num_execs=10, max_tree_depth=48, episodes=2048, eval_flag=False).execute()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XF5nFNtWacW_"
      },
      "source": [
        "Executer(env_name='merge-v0', num_execs=1, max_tree_depth=48, episodes=2048, eval_flag=True).execute()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQ011MIYatSb"
      },
      "source": [
        "Executer(env_name='intersection-v0', num_execs=10, max_tree_depth=20, episodes=2000, eval_flag=False).execute()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WB6e9TopbHuQ"
      },
      "source": [
        "Executer(env_name='intersection-v0', num_execs=1, max_tree_depth=20, episodes=2000, eval_flag=True).execute()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEfSbWieGiBu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b13b42f3-4382-42fa-f216-e79f1c1cbcf1"
      },
      "source": [
        "Executer(env_name='roundabout-v0', num_execs=10, max_tree_depth=32, episodes=786, eval_flag=False).execute()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "roundabout-v0\n",
            "\n",
            " execution number: 1 || Corresponding best action sequence: [3, 3, 3, 3, 3, 3, 2, 3, 1, 1, 0]\n",
            "\n",
            "  1   total reward:    10.920   average time:554.6 s\n",
            " execution number: 2 || Corresponding best action sequence: [0, 0, 3, 2, 2, 0]\n",
            "\n",
            "  2   total reward:     5.500   average time:283.9 s\n",
            " execution number: 3 || Corresponding best action sequence: [3, 0, 3, 3, 1, 3, 1, 1, 3, 1, 3]\n",
            "\n",
            "  3   total reward:     7.320   average time:374.9 s\n",
            " execution number: 4 || Corresponding best action sequence: [2, 3]\n",
            "\n",
            "  4   total reward:     5.510   average time:284.5 s\n",
            " execution number: 5 || Corresponding best action sequence: [3, 1, 3, 1, 1, 3, 3, 3, 3, 1, 3]\n",
            "\n",
            "  5   total reward:     6.608   average time:339.9 s\n",
            " execution number: 6 || Corresponding best action sequence: [2, 0, 3]\n",
            "\n",
            "  6   total reward:     5.520   average time:285.7 s\n",
            " execution number: 7 || Corresponding best action sequence: [0, 1, 3, 1, 4, 4, 0]\n",
            "\n",
            "  7   total reward:     4.743   average time:247.1 s\n",
            " execution number: 8 || Corresponding best action sequence: [0, 3, 3, 1, 3, 3, 3, 3, 1, 2, 1]\n",
            "\n",
            "  8   total reward:     5.485   average time:279.8 s\n",
            " execution number: 9 || Corresponding best action sequence: [0, 1, 2]\n",
            "\n",
            "  9   total reward:     4.884   average time:250.3 s\n",
            " execution number: 10 || Corresponding best action sequence: [3, 3, 3, 3, 3, 1, 1, 1, 3, 0, 1]\n",
            "\n",
            "  10   total reward: 5.492   average time: 273.74 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6pKbLevGiE3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97935ba3-200c-4e90-ee67-6fbcf8d139ae"
      },
      "source": [
        "Executer(env_name='roundabout-v0', num_execs=1, max_tree_depth=32, episodes=786, eval_flag=True).execute()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "roundabout-v0\n",
            "\n",
            " execution number: 1 || Corresponding best action sequence: [3, 3, 1, 3, 0, 3, 1, 1, 1, 3, 0]\n",
            "\n",
            "   1   total reward: 10.920   average time: 513.03 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3GNy7tX51Zl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}