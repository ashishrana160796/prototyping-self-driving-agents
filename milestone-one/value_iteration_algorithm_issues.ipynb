{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "value-iteration-algorithm-issues.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrWlDjxINk-q"
      },
      "source": [
        "# gym related import statements.\n",
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) # errors only\n",
        "import time\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEowmqZ5PtFT"
      },
      "source": [
        "### **Value Iteration Algorithm Implementation**\n",
        "\n",
        "__For environment `Taxi-v3` we have a model of state transition and reward probabilities available to us i.e. P[s][a] information is present for applying model-based learning approaches.__  \n",
        "\n",
        "__First, we discuss _Value Iteration Algorithm_ which randomly assigns values to `V(s)` and iteratively updates `Q(s,a)` and `V(s)` values until the convergence of the problem.__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5rXQ0FyPYT1"
      },
      "source": [
        "def execute_eps(env, policy, gamma=1.0, render=False):\n",
        "    obs = env.reset()\n",
        "    total_reward = 0\n",
        "    step_ind = 0\n",
        "    while True:\n",
        "        if render:\n",
        "            env.render()\n",
        "        obs, reward, done, _ = env.step(int(policy[obs]))\n",
        "        total_reward += (gamma ** step_ind * reward)\n",
        "        step_ind +=1\n",
        "        if done:\n",
        "            break\n",
        "    return total_reward"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vAlGYLXSLjw"
      },
      "source": [
        "def eval_policy(env, policy, gamma=1.0,  render=False, n=100):\n",
        "    score_values = [execute_eps(env, policy, gamma, render) for _ in range(n)]\n",
        "    return np.mean(score_values)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUoWDoI3SLnX"
      },
      "source": [
        "def determine_policy(env, v, gamma=1.0):\n",
        "    policy = np.zeros(env.nS)\n",
        "    for s in range(env.nS):\n",
        "        q_sa = np.zeros(env.action_space.n)\n",
        "        for a in range(env.action_space.n):\n",
        "            for next_sr in env.P[s][a]:\n",
        "                p, s_, r, _ = next_sr\n",
        "                q_sa[a] += (p * (r + gamma * v[s_]))\n",
        "        policy[s] = np.argmax(q_sa)\n",
        "    return policy"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYwrhvvHSLsK"
      },
      "source": [
        "def value_iteration(env, gamma=1.0):\n",
        "    value = np.zeros(env.nS)\n",
        "    max_iterations = 10000\n",
        "    eps = 1e-10\n",
        "    for i in range(max_iterations):\n",
        "        prev_v = np.copy(value)\n",
        "        q_sa = np.zeros((env.nS, env.nA))\n",
        "        for s in range(env.nS):\n",
        "            for a in range(env.nA):\n",
        "                for p, s_, r, _ in env.P[s][a]:\n",
        "                    q_sa[s][a] += p * (r + gamma * prev_v[s_])\n",
        "        value = np.max(q_sa, axis=1)\n",
        "        if (np.sum(np.fabs(prev_v - value)) <= eps):\n",
        "            print('Problem converged at iteration %d.' % (i + 1))\n",
        "            break\n",
        "    return value"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSpeyR0kSL8I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d466829b-c7fe-459f-d3b8-0b79b3f12f40"
      },
      "source": [
        "gamma = 1.00 # not caring about future rewards\n",
        "env = gym.make('FrozenLake8x8-v0')\n",
        "optimal_value_func = value_iteration(env, gamma)\n",
        "start_time = time.time()\n",
        "policy = determine_policy(env, optimal_value_func, gamma)\n",
        "policy_score = eval_policy(env, policy, gamma, False, n=1000)\n",
        "end_time = time.time()\n",
        "print(\"Best Policy Score = %0.2f and Time taken = %4.4f seconds\" % (np.mean(policy_score),\n",
        "end_time - start_time))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Problem converged at iteration 1599.\n",
            "Best Policy Score = 0.90 and Time taken = 1.3020 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfMIP7gnYeUY",
        "outputId": "de226d08-58fc-4674-98c6-1c56815d790d"
      },
      "source": [
        "gamma = 0.99 # caring about future rewards\n",
        "env = gym.make('FrozenLake8x8-v0')\n",
        "optimal_value_func = value_iteration(env, gamma)\n",
        "start_time = time.time()\n",
        "policy = determine_policy(env, optimal_value_func, gamma)\n",
        "policy_score = eval_policy(env, policy, gamma, False, n=1000)\n",
        "end_time = time.time()\n",
        "print(\"Best Policy Score = %0.2f and Time taken = %4.4f seconds\" % (np.mean(policy_score),\n",
        "end_time - start_time))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Problem converged at iteration 750.\n",
            "Best Policy Score = 0.40 and Time taken = 1.3463 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhAdlxGjSL6Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a83c38f2-c95d-47f1-fbf6-8b644225cf2b"
      },
      "source": [
        "gamma = 1.00 # not caring about future rewards\n",
        "env = gym.make(\"Taxi-v3\")\n",
        "optimal_value_func = value_iteration(env, gamma)\n",
        "start_time = time.time()\n",
        "policy = determine_policy(env, optimal_value_func, gamma)\n",
        "policy_score = eval_policy(env, policy, gamma, False, n=1000)\n",
        "end_time = time.time()\n",
        "print(\"Best Policy Score = %0.2f and Time taken = %4.4f seconds\" % (np.mean(policy_score),\n",
        "end_time - start_time))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best Policy Score = -195.38 and Time taken = 2.7493 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZJwgiXwNlDz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffc9366a-c74c-40cb-dee9-0f43c7da7b61"
      },
      "source": [
        "gamma = 0.99 # caring about future rewards\n",
        "env = gym.make(\"Taxi-v3\")\n",
        "optimal_value_func = value_iteration(env, gamma)\n",
        "start_time = time.time()\n",
        "policy = determine_policy(env, optimal_value_func, gamma)\n",
        "policy_score = eval_policy(env, policy, gamma, False, n=1000)\n",
        "end_time = time.time()\n",
        "print(\"Best Policy Score = %0.2f and Time taken = %4.4f seconds\" % (np.mean(policy_score),\n",
        "end_time - start_time))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Problem converged at iteration 3125.\n",
            "Best Policy Score = 6.38 and Time taken = 0.2815 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}