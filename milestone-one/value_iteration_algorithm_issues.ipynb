{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "value-iteration-algorithm-issues.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrWlDjxINk-q"
      },
      "source": [
        "# gym related import statements.\n",
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) # errors only\n",
        "import time\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEowmqZ5PtFT"
      },
      "source": [
        "### **Value Iteration Algorithm Implementation**\n",
        "\n",
        "__For environment `Taxi-v3` we have a model of state transition and reward probabilities available to us i.e. P[s][a] information is present for applying model-based learning approaches.__  \n",
        "\n",
        "__First, we discuss _Value Iteration Algorithm_ which randomly assigns values to `V(s)` and iteratively updates `Q(s,a)` and `V(s)` values until the convergence of the problem.__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5rXQ0FyPYT1"
      },
      "source": [
        "def execute_eps(env, policy, gamma=1.0, render=False):\n",
        "    obs = env.reset()\n",
        "    total_reward = 0\n",
        "    step_ind = 0\n",
        "    while True:\n",
        "        if render:\n",
        "            env.render()\n",
        "        obs, reward, done, _ = env.step(int(policy[obs]))\n",
        "        total_reward += (gamma ** step_ind * reward)\n",
        "        step_ind +=1\n",
        "        if done:\n",
        "            break\n",
        "    return total_reward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vAlGYLXSLjw"
      },
      "source": [
        "def eval_policy(env, policy, gamma=1.0,  render=False, n=100):\n",
        "    score_values = [execute_eps(env, policy, gamma, render) for _ in range(n)]\n",
        "    return np.mean(score_values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUoWDoI3SLnX"
      },
      "source": [
        "def determine_policy(v, gamma=1.0):\n",
        "    policy = np.zeros(env.nS)\n",
        "    for s in range(env.nS):\n",
        "        q_sa = np.zeros(env.action_space.n)\n",
        "        for a in range(env.action_space.n):\n",
        "            for next_sr in env.P[s][a]:\n",
        "                p, s_, r, _ = next_sr\n",
        "                q_sa[a] += (p * (r + gamma * v[s_]))\n",
        "        policy[s] = np.argmax(q_sa)\n",
        "    return policy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYwrhvvHSLsK"
      },
      "source": [
        "def value_iteration(env, gamma=1.0):\n",
        "    value = np.zeros(env.nS)\n",
        "    max_iterations = 10000\n",
        "    eps = 1e-10\n",
        "    for i in range(max_iterations):\n",
        "        prev_v = np.copy(value)\n",
        "        for s in range(env.nS):\n",
        "            q_sa = [sum([p * (r + prev_v[s_]) for p, s_, r, _ in env.P[s][a]]) for a in range(env.nA)]\n",
        "            value[s] = max(q_sa)\n",
        "        if (np.sum(np.fabs(prev_v - value)) <= eps):\n",
        "            print('Problem converged at iteration %d.' % (i + 1))\n",
        "            break\n",
        "    return value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSpeyR0kSL8I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "13203840-c1b6-401d-f21e-62a2d5d804ef"
      },
      "source": [
        "gamma = 1.0\n",
        "env = gym.make('FrozenLake8x8-v0')\n",
        "optimal_value_func = value_iteration(env, gamma)\n",
        "start_time = time.time()\n",
        "policy = determine_policy(optimal_value_func, gamma)\n",
        "policy_score = eval_policy(env, policy, gamma, False, n=1000)\n",
        "end_time = time.time()\n",
        "print(\"Best Policy Score = %0.2f and Time taken = %4.4f seconds\" % (np.mean(policy_score),\n",
        "end_time - start_time))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Problem converged at iteration 1599.\n",
            "Best Policy Score = 0.87 and Time taken = 1.0837 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhAdlxGjSL6Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "272375f6-2244-4c1a-ab73-0c971a402eb4"
      },
      "source": [
        "gamma = 1.0\n",
        "env = gym.make(\"Taxi-v3\")\n",
        "optimal_value_func = value_iteration(env, gamma)\n",
        "start_time = time.time()\n",
        "policy = determine_policy(optimal_value_func, gamma)\n",
        "policy_score = eval_policy(env, policy, gamma, False, n=1000)\n",
        "end_time = time.time()\n",
        "print(\"Best Policy Score = %0.2f and Time taken = %4.4f seconds\" % (np.mean(policy_score),\n",
        "end_time - start_time))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best Policy Score = -197.70 and Time taken = 1.8199 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZJwgiXwNlDz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}