{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "monte-carlo-tree-search-taxi-v3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74b8C7OooUCQ",
        "colab_type": "text"
      },
      "source": [
        "### **Monte Carlo Tree Search Implementation**\n",
        "\n",
        "**In this notebook we implement Upper Confidence Bound (UCB) based MCTS algorithm for performing the stated tasks in a given environment.**  \n",
        "  \n",
        "**We revisit `Taxi-v3` environment for implementing this agent. Also, we experiment on different max-depth values for a given MCTS tree values and analyze its effect on the performance of the agent.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzI1tH05q8y0",
        "colab_type": "text"
      },
      "source": [
        "### **Monte Carlo Tree Search Component Declarations**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pF3PXogj3Wv1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import gym\n",
        "import sys\n",
        "import random\n",
        "import itertools\n",
        "from time import time\n",
        "from copy import copy\n",
        "from math import sqrt, log"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_S51G4Cfq4rt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining node class and associated properties with it.\n",
        "class Node:\n",
        "    def __init__(self, parent, action):\n",
        "        self.parent = parent\n",
        "        self.action = action\n",
        "        self.children = []\n",
        "        self.explored_children = 0\n",
        "        self.visits = 0\n",
        "        self.value = 0"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrICwvfpq4p4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This function determine complete exhaustive list of all the nodes.\n",
        "def node_expansion(space):\n",
        "    if isinstance(space, gym.spaces.Discrete):\n",
        "        return range(space.n)\n",
        "    elif isinstance(space, gym.spaces.Tuple):\n",
        "        return itertools.product(*[node_expansion(s) for s in space.spaces])\n",
        "    else:\n",
        "        raise NotImplementedError"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXOqsiaMq4na",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Upper Confidence Bound U(s,a) calculation formula.\n",
        "def upper_conf_bound(node):\n",
        "    return node.value / node.visits + sqrt(log(node.parent.visits)/node.visits)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xz4jodm1q4lu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def moving_averages(v, n):\n",
        "    n = min(len(v), n)\n",
        "    ret = [.0]*(len(v)-n+1)\n",
        "    ret[0] = float(sum(v[:n]))/n\n",
        "    for i in range(len(v)-n):\n",
        "        ret[i+1] = ret[i] + float(v[n+i] - v[i])/n\n",
        "    return ret"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbhkxK9iq4jK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# MCTS denotes Monte Carlo Tree Search\n",
        "class MCTS:\n",
        "    def __init__(self, env_name, num_execs=300, max_tree_depth=1000, episodes=10000):\n",
        "        self.env_name = env_name\n",
        "\n",
        "        self.num_execs = num_execs\n",
        "        self.max_tree_depth = max_tree_depth\n",
        "        self.episodes = episodes\n",
        "    \n",
        "    def print_stats(self, num_exec, score, avg_time):\n",
        "        sys.stdout.write('execution number: \\r%3d   total reward:%10.3f   average time:%4.1f s' % (num_exec, score, avg_time))\n",
        "        sys.stdout.flush()\n",
        "        if (num_exec % 10) == 0:\n",
        "            print(\"execution number: \\r%4d   total reward: %4.3f   average time: %4.2f s\" % (num_exec, score, avg_time))\n",
        "\n",
        "    def execute(self):\n",
        "        print(self.env_name)\n",
        "        # For maintaining list of best rewards.\n",
        "        best_rewards = []\n",
        "        start_time = time()\n",
        "        env = gym.make(self.env_name)\n",
        " \n",
        "        for loop in range(self.num_execs):\n",
        "            env.reset()\n",
        "            root = Node(None, None)\n",
        "            # For capturing list of best actions taken by the agent.\n",
        "            best_actions = []\n",
        "            best_reward = float(\"-inf\")\n",
        "\n",
        "            for _ in range(self.episodes):\n",
        "                state = copy(env)\n",
        "\n",
        "                sum_reward = 0\n",
        "                node = root\n",
        "                terminal = False\n",
        "                actions = []\n",
        "\n",
        "                # selection of suitable node children\n",
        "                while node.children:\n",
        "                    if node.explored_children < len(node.children):\n",
        "                        child = node.children[node.explored_children]\n",
        "                        node.explored_children += 1\n",
        "                        node = child\n",
        "                    else:\n",
        "                        node = max(node.children, key=upper_conf_bound)\n",
        "                    _, reward, terminal, _ = state.step(node.action)\n",
        "                    sum_reward += reward\n",
        "                    actions.append(node.action)\n",
        "\n",
        "                # expansion of all the children nodes\n",
        "                if not terminal:\n",
        "                    node.children = [Node(node, a) for a in node_expansion(state.action_space)]\n",
        "                    random.shuffle(node.children)\n",
        "\n",
        "                # creating exhaustive list of actions\n",
        "                while not terminal:\n",
        "                    action = state.action_space.sample()\n",
        "                    _, reward, terminal, _ = state.step(action)\n",
        "                    sum_reward += reward\n",
        "                    actions.append(action)\n",
        "\n",
        "                    if len(actions) > self.max_tree_depth:\n",
        "                        sum_reward -= 100\n",
        "                        break\n",
        "\n",
        "                # retaining the best reward value and actions\n",
        "                if best_reward < sum_reward:\n",
        "                    best_reward = sum_reward\n",
        "                    best_actions = actions\n",
        "\n",
        "                # backpropagating in MCTS for assigning reward value to a node.\n",
        "                while node:\n",
        "                    node.visits += 1\n",
        "                    node.value += sum_reward\n",
        "                    node = node.parent\n",
        "\n",
        "                \n",
        "            sum_reward = 0\n",
        "            for action in best_actions:\n",
        "                _, reward, terminal, _ = env.step(action)\n",
        "                sum_reward += reward\n",
        "                if terminal:\n",
        "                    break\n",
        "\n",
        "            best_rewards.append(sum_reward)\n",
        "            score = max(moving_averages(best_rewards, 100))\n",
        "            avg_time = (time() - start_time) / (loop + 1)\n",
        "            self.print_stats(loop + 1, score, avg_time)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VLF2k8o1KX6",
        "colab_type": "text"
      },
      "source": [
        "### **Executing MCTS for `Taxi-v3 ` and reward function calculation.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zC4O25R1q4ef",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "    MCTS('Taxi-v3', num_execs=200, max_tree_depth=512, episodes=5000).execute()"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpTcupenc9E1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "outputId": "d7f5dcaa-ea7c-44ac-8ebc-a57f9eb8f107"
      },
      "source": [
        "# Printing average achieved score by the MCTS algorithm in Taxi-v3 environment\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Taxi-v3\n",
            "  10   total reward: 0.100   average time: 2.08 s\n",
            "  20   total reward: 0.900   average time: 1.98 s\n",
            "  30   total reward: 3.600   average time: 2.19 s\n",
            "  40   total reward: 3.700   average time: 2.05 s\n",
            "  50   total reward: 4.360   average time: 2.12 s\n",
            "  60   total reward: 2.483   average time: 2.06 s\n",
            "  70   total reward: 3.414   average time: 1.97 s\n",
            "  80   total reward: 3.987   average time: 1.91 s\n",
            "  90   total reward: 3.867   average time: 1.87 s\n",
            " 100   total reward: 4.720   average time: 1.79 s\n",
            " 110   total reward: 5.780   average time: 1.80 s\n",
            " 120   total reward: 5.870   average time: 1.79 s\n",
            " 130   total reward: 5.870   average time: 1.79 s\n",
            " 140   total reward: 5.870   average time: 1.79 s\n",
            " 150   total reward: 5.870   average time: 1.78 s\n",
            " 160   total reward: 5.870   average time: 1.80 s\n",
            " 170   total reward: 5.870   average time: 1.78 s\n",
            " 180   total reward: 5.870   average time: 1.85 s\n",
            " 190   total reward: 5.870   average time: 1.89 s\n",
            " 200   total reward: 5.870   average time: 1.85 s\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}